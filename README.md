# Tokenization-NLP
In this repo, implementation of various Tokenizers takes place along with the usage of 'nltk' library

Itâ€™s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.
Tokenization is a way of separating a piece of text into smaller units called tokens. 

Byte Pair Encoding (BPE) is a widely used tokenization method among transformer-based models. 
BPE is a word segmentation algorithm that merges the most frequently occurring character or character sequences iteratively. Here is a step by step guide to learn BPE.
